#CUDA 기초 1

##간단한 CUDA 프로그램
간단한 CUDA 프로그램을 작성하면서 시작해 봅시다.
크기가 1024인 두 개의 1차원 배열에 대해 두 배열의 합 배열을 구하는 프로그램을
작성해 봅시다.
우선 CUDA를 사용하지 않은 일반적인 프로그램을 작성한다면
\refpro{p_va_ser}\와 같을 것입니다.

\begin{program}
	\caption{순차 벡터 합}
	\label{p_va_ser}
	\lstinputlisting[style=book_ex]{example_codes/chap1/p_va_ser.cpp}
	%\includegraphics[scale=0.7]{algorithm/figures/p_big1.pdf}
\end{program} 

위 프로그램에서 7, 8번 줄에서는 배열 a와 b에 공간을 할당하고,
10--13번 줄에서는 두 배열을 적당한 값들로 채웁니다.
그리고 15--17번 줄에서 두 배열의 합을 배열 c에 저장합니다.
마지막으로 19--21번 줄에서는 배열 c의 값을 출력하지요.

이 프로그램을 이제는 CUDA를 이용해서 여러분의 그래픽 카드에서 실행되도록
바꾸어 보겠습니다.
CUDA로 프로그램을 바꿀 때 먼저 할 일은 프로그램의 어느 부분을
GPU에서 실행할지 결정하는 것입니다.
이 프로그램에서는 15--17번 줄에 있는, 배열을 합하는 부분을
그래픽 카드에서 실행하는 것이 나을 것 같습니다.
그럼 15--17번 줄의 내용을 GPU에서 실행하라고 표시를 해야 할 텐데,
CUDA에서는 GPU에서 실행할 부분을 별개의 함수로 작성하고
그 함수의 앞에 \texttt{\_\_global\_\_}이라고 적음으로써 구별합니다.
즉, 여러분이 알고 있는 방법으로 작성한 일반적인 함수는 CPU에서 동작하고
함수 앞에 \texttt{\_\_global\_\_}이라고 되어 있는 함수는
GPU에서 동작하는 것입니다.
15--17번 줄을 대신할 함수의 이름을 \texttt{vec\_add}라고 한다면 함수의 겉모양이
다음과 같을 것입니다.

\begin{lstlisting}[frame=single]
__global__ void vec_add(int *a, int *b, int *c) {

}
\end{lstlisting}

이제 \texttt{vec\_add} 함수의 내부를 채워 봅시다.
이 때 여러분이 하나 아셔야 하는 것이 쓰레드(thread)라는 개념입니다.
아시다시피 CUDA는 병렬 프로그래밍 환경입니다.
즉 GPU에 있는 여러 개의 코어가 프로그램을 동시에 실행하는 것이지요.
이렇게 병렬적으로 동시에 실행하는 기준이 쓰레드입니다.
동일한 동작을 하는 여러 개의 쓰레드가 여러 개의 코어에서 동시에 실행되며,
하나의 쓰레드 내에서는 프로그램이 직렬적으로 수행됩니다.
개념적으로 보면, 앞에서 작성한 \texttt{vec\_add}라는 함수 내부의 코드도
쓰레드 개수 만큼 만들어져서 여러 개의 코어에서 각각을 병렬적으로 수행합니다.
그러나 한 쓰레드 내에서는 코드는 직렬적으로 수행됩니다.
지금 하고 있는 예제에서는 쓰레드가 수행할 내용이 두 배열에 있는
대용하는 배열 요소를 더하는 것이므로 각각의 쓰레드가 수행할 작업은
아래와 같이 덧셈일 것이고, 이러한 쓰레드가 여러 개 만들어져서
여러 개의 코어에서 동시에 덧셈을 수행할 것입니다.

\begin{lstlisting}[frame=single]
__global__ void vec_add(int *a, int *b, int *c) {

	c[tid] = a[tid] + b[tid];
}
\end{lstlisting}

그런데 쓰레드들이 동일한 코드를 가지고 동일한 동작을 한다고 해서
완전히 똑같은 동작을 한다면 여러 개의 쓰레드를 만들어서 여러 번 할 필요가
없을 것입니다.
예를 들어 위의 코드에서 쓰레드들 사이에 똑같은 tid 값을 가지고 작업을
수행한다면 여러 개의 쓰레드들을 수행하는 것이 의미가 없을 것입니다.
같은 동작을 하기는 하는데 서로 다른 데이터에 대해
같은 동작을 하는 것이 의미가 있을 것입니다.
위 예제에서는 각 쓰레드마다 서로 다른 tid 값을 가져야 서로 다른
배열 요소들을 더해서 배열 c에 저장할 것입니다.
CUDA에서는 이를 위해 각 쓰레드 마다 서로 다른 번호를 부여하고,
이 번호는 \texttt{threadIdx.x}를 통해 알 수 있습니다.
만일, 1024개의 쓰레드들을 생성한다면, 각 쓰레드에는 0--1024의 번호가 부여되고,
23번 쓰레드에서 \texttt{threadIdx.x}를 참조하면 23이라는 값을
얻을 수 있다는 것입니다.
이 \texttt{threadIdx.x}를 이용해서 tid 값을 아래와 같이 정할 수 있습니다.

\begin{lstlisting}[frame=single]
__global__ void vec_add(int *a, int *b, int *c) {
	int	tid = threadIdx.x;
	c[tid] = a[tid] + b[tid];
}
\end{lstlisting}

이렇게 하면 각 쓰레드들은 서로 다른 tid 값을 가질 것입니다.
어떤 쓰레드에 23번 번호가 부여되었다면, 그 쓰레드에서 tid 변수의 값은 23이
되고 따라서 \texttt{c[23] = a[23] + b[23]}을 수행할 것입니다.
즉 23번 배열 요소의 덧셈을 합니다.
비슷하게 번호가 100번이 쓰레드는 100번 배열 요소에 대해 덧셈을 수행할 것입니다.
1024개의 쓰레드가 있다면 각 쓰레드에 0번에서 1023번의 번호가 부여될 테고,
따라서 각 쓰레드는 \texttt{c[0] = a[0] + b[0]} --
\texttt{c[1023] = a[1023] + b[1023]}을 
수행할 것입니다.
그리고 GPU에서는 이 1024개의 쓰레드가 동시에 병렬적으로 수행되는 것입니다.

그럼 이제 main에서 이 함수를 호출해야 할 것입니다. 호출은 아래와 같이 합니다.

\begin{lstlisting}[frame=single]
	vec_add<<<1, 1024>>>(da, db, dc);
\end{lstlisting}

일반적인 함수 호출에 \texttt{<<<1, 1024>>>}가 추가되었습니다.
이 부분이 몇 개의 쓰레드를 만들지를 지정하는 부분입니다.
앞에 있는 1은 조금 뒤에 설명하고, 뒤에 있는 1024가 1024개의 쓰레드를 
생성하라고 지정합니다.

그런데 함수의 인자가 좀 이상합니다. 앞서 \refpro{p_va_ser}에 있던
포인터 a, b, c가 아니라 da, db, dc입니다.
이 것은 쓰레드에서 코드가 수행될 때 어느 메모리에 있는 데이터에 대해
수행되는지를 생각해 보면 이유를 알 수 있을 것입니다.
쓰레드는 GPU에서 수행됩니다. 따라서 쓰레드가 동작하는 데이터들은
GPU의 메모리에 있어야 될 것입니다.
그런데 \refpro{p_va_ser}의 a, b, c 포인터가 가리키는 영역은
CPU에서 수행하는 코드에서 \texttt{malloc}을 통해 할당받았으므로
CPU의 메모리에 있는 영역입니다.
즉 GPU에서 a, b, c 포인터가 가리키는 영역은 접근할 수 없습니다.
쓰레드에서 이 데이터를 처리하려면 우선 GPU의 메모리에 영역을 할당받고
그 영역으로 데이터를 옮겨야 합니다.
이 부분을 추가하면 아래와 같습니다.

\begin{lstlisting}[frame=single]
	int	*da, *db, *dc;
	cudaMalloc((void **)&da, sizeof(int) * NUM_DATA);
	cudaMalloc((void **)&db, sizeof(int) * NUM_DATA);
	cudaMalloc((void **)&dc, sizeof(int) * NUM_DATA);

	cudaMemcpy(da, a, sizeof(int)*NUM_DATA, cudaMemcpyHostToDevice);
	cudaMemcpy(db, b, sizeof(int)*NUM_DATA, cudaMemcpyHostToDevice);
	vec_add<<<1, 1024>>>(da, db, dc);
	cudaMemcpy(c, dc, sizeof(int)*NUM_DATA, cudaMemcpyDeviceToHost);
\end{lstlisting}

2--4번 줄이 GPU 메모리에서 일정 영역을 할당받는 부분입니다.
C언어의 \texttt{malloc} 함수와 비슷한 \texttt{cudaMalloc}이라는
함수로 할당을 받으며
첫 번째 인자는 할당받을 영역의 주소를 저장할 포인터의 주소이고
두 번째 인자는 할당받을 영역의 크기(바이트 단위)입니다.
2--4번을 수행하면 \texttt{NUM\_DATA}개의 int 값을 저장할 수 있는 영역 세 개를
 GPU 메모리에서 할당받아서 그 주소가 각각 da, db, dc에 저장됩니다.

다음으로 6, 7번 줄에서 배열 a와 b---실제로는 포인터 a와 b가 가리키는 영역---에
저장된 데이터를 da와 db가 가리키는 영역으로 복사합니다.
세 번째 인자가 복사할 데이터의 양(바이트 단위)이고
마지막 인자가 복사하는 방향을 가리킵니다.
6, 7번 줄과 같이 \texttt{cudaMemcpyHostToDevice}라고 하면 CPU의 메모리에서
GPU의 메모리로 복사되며,
\texttt{cudaMemcpyDeviceToHost}라고 하면
GPU의 메모리에서 CPU의 메모리로 복사됩니다.

여기까지 하면 덧셈을 수행할 데이터들이 GPU의 메모리로 옮겨졌으니
8번 줄과 같이 \texttt{vec\_add} 함수를 수행하면 됩니다.
그러면 da, db가 가리키는 GPU 메모리 영역에 저장되어 있는 데이터를
읽어서 더한 뒤 dc가 가리키는 GPU 메모리 영역에 저장합니다.
마지막으로 9번 줄에서 dc에 저장된 데이터를 c가 가리키는 영역으로 옮깁니다.
이렇게 되면 a와 b에 저장된 값들을 더한 결과가 c에 저장되겠지요.

정리해서 코드를 완성하면 \refpro{p_va_par}\과 같습니다.
반복문을 이용하여 배열 a와 b의 합을 구하는 부분이
GPU에서 실행될 함수 \texttt{vec\_add}를 호출하는 부분으로 바뀌었으며,
이를 위해서 GPU의 메모리에 영역을 할당받고
입/출력 데이터를 옮기는 코드도 추가되었습니다.
마지막에는 GPU 메모리에 할당받은 영역을 반환하는 cudaFree도 추가하였고,
이러한 CUDA에 관련된 함수들을 사용하기 위해 \texttt{cuda.h}를
include하는 문장도 프로그램의 처음 부분에 넣었습니다.

\begin{program}
	\caption{병렬 벡터 합}
	\label{p_va_par}
	\lstinputlisting[style=book_ex]{example_codes/chap1/p_va_par.cu}
	%\includegraphics[scale=0.7]{algorithm/figures/p_big1.pdf}
\end{program} 

##쓰레드와 쓰레드 블록
\refpro{p_va_par}에서는 크기가 1024인 두 1차원 배열의 합을
1024개의 쓰레드를 생성해서 병렬적으로 계산하였습니다.
크기가 더 큰 두 1차원 배열의 합은 어떻게 처리할까요?
예를 들어 크기가 512$\times$1024인 배열을 생각해 봅시다.
간단히 하려면 앞의 예제 프로그램에서 \texttt{malloc}과 \texttt{cudaMalloc}을
할 때 더 큰 영역을 할당받고
\texttt{vec\_add}를 호출할 때 생성되는 쓰레드의 개수를 512$\times$1024개로,
즉 \texttt{vec\_add<<<1,512*1024>>>(...)}라고 하면 될 것 같습니다.
그런데 이 방법은 별로 선호되는 방법도 아니고
작성해서 실행해 보면 에러가 발생합니다.
왜 그런지는 다음 장에서 설명하고, 여기서는 보통 많이 사용되는 방법에 대해
얘기하겠습니다.

CUDA에서는 쓰레드를 생성할 때 쓰레드들이 개벌적으로 생성되지 않고
쓰레드가 여러 개 묶여 있는 쓰레드 블록(thread block)이라는 단위로 생성됩니다.
예를 들어, 쓰레드 블록을 8개 생성하는데 각 블록은 128개의 쓰레드를 포함한다고
지정한다면, 생성되는 쓰레드의 총 개수는 8$\times$128 = 1024개가 되는 것입니다.
이렇게 실제로 지정하는 것은 쓰레드 블록의 개수와 한 블록 당 쓰레드 개수이며,
이 숫자들이 앞서 \texttt{vec\_acc<<<1,1024>>>(...)}에서
1과 1024의 진짜 의미입니다.
즉 \texttt{vec\_acc<<<1,1024>>>(...)}이라고 호출하면 쓰레드를 1024개 가지는
쓰레드 블록을 1개 생성하는 것입니다.
\texttt{vec\_acc<<<3,128>>>(...)}이라고 호출하면 쓰레드를 128개 가지는
쓰레드 블록을 3개 생성해서, 생성되는 쓰레드의 총 개수는 3$\times$128 = 384개가
되겠지요.

그런데 쓰레드 블록을 사용하면 각 쓰레드에게 부여되는 번호는 어떻게 되는지
궁금해집니다.
우선 각 쓰레드의 번호는 블록 내에서 부여됩니다.
한 블록의 쓰레드들에 대해서도 번호가 0, 1, 2, ...으로 부여되고
다른 블록의 쓰레드들에게도 0, 1, 2, ...이라는 번호가 부여됩니다.
대신 블록별로 서로 다른 블록 번호가 부여됩니다.
예를 들어, \texttt{vec\_acc<<<3,128>>>(...)}에 의해 생성되는
세 개의 쓰레드 블록에는 각각 0, 1, 2번이라는 블록 번호가 부여됩니다.
그리고 각 쓰레드 블록 내의 쓰레드들에게는 0, 1, 2, ..., 127이라는 쓰레드
번호가 부여됩니다.
쓰레드 번호는 앞에서 설명했듯이 \texttt{threadIdx.x}를 통해 알 수 있고
블록 번호는 비슷하게 \texttt{blockIdx.x}를 통해 알 수 있습니다.
그리고 한 블록 내에 속한 쓰레드의 개수(블록 크기라고 합시다.)를 알고 싶을 때는
\texttt{blockDim.x}를 사용하면 됩니다.

이 값들, 즉 쓰레드 번호와 블록 번호, 블록 크기를 조합하면
모든 쓰레드들에 고유하고 연속적인 번호를 만들 수 있습니다.
대표적인 방법이 아래와 같이 계산하는 것입니다.

\begin{lstlisting}[frame=single]
	int	tid = blockIdx.x*blockDim.x + threadIdx.x;
\end{lstlisting}

예를 들어, \texttt{vec\_acc<<<3,128>>>(...)}이라고 호출했을 때
	각 쓰레드에서 tid가 얼마가 될지 알아 봅시다.
생성되는 세 개의 쓰레드 블록에는 0, 1, 2의 번호가 부여되고
쓰레드 블록들 각각에서는 0, 1, 2, ..., 127의 번호가 쓰레드들에 부여됩니다.
쓰레드 블록당 쓰레드 개수가 128이므로 \texttt{blockDim.x}는 128이 되겠지요.
이 번호와 값들로 각 쓰레드에서의 tid를 계산해 보면,
0번 쓰레드 블록의 쓰레드들에서는 \texttt{blockIdx.x}가 0이므로
	tid는 0, 1, 2, ..., 127이 될 것입니다.
1번 쓰레드 블록의 경우 \texttt{blockIdx.x}가 1이므로
	tid는 128, 129, 130, ..., 255가 되고,
2번 쓰레드 블록에서는 \texttt{blockIdx.x}가 2이므로
	tid는 256, 257, 258, ..., 383이 됩니다.
쓰레드들을 모두 모아보면, 그 안에서 tid의 값들은 0, 1, 2, ..., 383의
연속적이고 고유한 값들이 됩니다.

이제 문제는 간단해졌습니다.
512$\times$1024개의 쓰레드를 생성하고 싶다면
\texttt{vec\_add<<<x,y>>>(...)}에서 x와 y의 곱이 512$\times$1024가 되면 됩니다.
\texttt{vec\_add<<<512,1024>>>(...)}라고 해도 되고
\texttt{vec\_add<<<2048,256>>>(...)}라고 해도 됩니다.
그리고 각 쓰레드들이 처리할 배열의 첨자는 앞에서와 같이 \texttt{blockIdx.x}와
\texttt{blockDim.x}, \texttt{threadIdx.x}를 이용하여 구하면 됩니다.
\refpro{p_va_par2}\가 이런 방식으로 크기가 512$\times$1024인 두 배열의
합을 구하도록 프로그램입니다.

\begin{program}
	\caption{병렬 벡터 합 2}
	\label{p_va_par2}
	\lstinputlisting[style=book_ex]{example_codes/chap1/p_va_par2.cu}
	%\includegraphics[scale=0.7]{algorithm/figures/p_big1.pdf}
\end{program} 

%여기서 for 반복문이 없다는 것도 보시기 바랍니다.
%1024개의 쓰레들을 만들어서 각 쓰레드들이 하나의 데이터에 대해 덧셈을
%하도록 작성할 예정이어서 각 쓰레드 안에서는 반복할 필요가 없습니다.

##호스트와 디바이스, 커널
CUDA 프로그램에서 많이 사용되는 용어 몇가지만 언급하겠습니다.
CUDA 프로그램에서는 CPU쪽을 호스트(host), GPU쪽을 디바이스(device)라고 합니다.
\refpro{p_va_par}\과 \refpro{p_va_par2}에서 \texttt{cudaMalloc}을 할 때
\texttt{cudaMemcpyHostToDevice}라고 지정하면 CPU쪽의 메모리에서
GPU쪽의 메모리로 데이터를 옮긴다고 했는데,
이 것 역시 CPU쪽을 호스트, GPU쪽을 디바이스로 부른다는 것을 안다면
금방 이해하실 수 있으실 것입니다.

그리고 CUDA 프로그램에서 함수들은 크게 두가지로 분류됩니다.
호스트에서 실행되는 함수와 디바이스에서 실행되는 함수입니다.
우리가 일반적인 C나 C++ 프로그램에서와 같이 작성하는 함수는 호스트에서
실행되는 함수이고, 함수의 앞에 \texttt{\_\_global\_\_}을 넣으면
디바이스에 실행되는 함수가 됩니다.
그리고 이렇게 디바이스에 실행되는 함수를 CUDA에서는 커널(kernel)이라고
지칭합니다.
마지막으로 GPU에서 CPU쪽으로 직접 값을 전달하는 것은 좀 어려운 면이 있어서
커널의 반환값 자료형은 반드시 \texttt{void}, 즉 반환값이 없어야 합니다.


